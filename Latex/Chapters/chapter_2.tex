\chapter{Framework}
\section{Introduction to Machine Learning}

Machine learning is a powerful tool that can be used to identify patterns in complex datasets. In the context of particle physics, machine learning algorithms can be used to detect signals from background noise in large datasets generated by detectors. In particular, for the detection of IBD signals from background, machine learning algorithms can be used to identify patterns in the data that are indicative of an IBD event, and to distinguish these signals from the background noise. Moreover, one advantage of machine learning for particle physics is that it can handle large amounts of data and identify subtle patterns that may be difficult for humans to detect.
\\

There are several machine learning algorithms that can be used for this purpose, including decision trees, deep neural networks and support vector machines. These algorithms can be trained on simulated data to recognize IBD events signals over the background. 


\subsection{Supervised Learning}

In supervised learning, the algorithm is trained on a labeled dataset, where the input data is accompanied by the correct output. The goal of the algorithm is to learn a function that can map input data to output data. Some examples of supervised learning algorithms include linear regression, logistic regression, decision trees, and support vector machines.

To understand the consepts of supervised learning it is useful to discuss a simle machin learning algorithm, linear regression. 

\section{Linear Regression}
Linear regression is a type of supervised learning algorithm used in machine learning for predictive analysis. It is used to model the relationship between a dependent variable (also called the target variable) and one or more independent variables (also called the features or predictors).

The basic idea behind linear regression is to find the best-fitting line that describes the relationship between the independent and dependent variables. This line is often called the regression line or the line of best fit. The equation of this line can be written as:

\begin{equation}
	y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p + \epsilon
\end{equation}

where $y$ is the dependent variable, $x_1, x_2, ..., x_p$ are the independent variables, $\beta_0, \beta_1, \beta_2, ..., \beta_p$ are the coefficients or parameters of the model, and $\epsilon$ is the error term. The error term captures the deviation of the actual values of the dependent variable from the predicted values.

In order to determine the values of the coefficients $\beta_0, \beta_1, \beta_2, ..., \beta_p$, a common approach is to minimize a loss function, which measures the difference between the predicted values of the dependent variable and the actual values. The most commonly used loss function in linear regression is the mean squared error (MSE) function, which is defined as:

\begin{equation}
	L(\beta_0, \beta_1, \beta_2, ..., \beta_p) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2
\end{equation}

where $n$ is the number of observations, $y_i$ is the actual value of the dependent variable for the $i$-th observation, and $\hat{y_i}$ is the predicted value of the dependent variable for the $i$-th observation.

The goal of linear regression is to find the values of the coefficients $\beta_0, \beta_1, \beta_2, ..., \beta_p$ that minimize the loss function $L(\beta_0, \beta_1, \beta_2, ..., \beta_p)$. This can be achieved using various optimization techniques such as gradient descent or normal equations.

However, it is important to note that linear regression can suffer from overfitting or underfitting. Overfitting occurs when the model is too complex and captures noise in the data, while underfitting occurs when the model is too simple and fails to capture the underlying patterns in the data. To prevent overfitting or underfitting, regularization techniques such as Ridge regression or Lasso regression can be used.


